[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "",
    "text": "Abstract.\nKeywords. svars, impulse responses, quarto, R, monetary policy"
  },
  {
    "objectID": "index.html#a-simple-example",
    "href": "index.html#a-simple-example",
    "title": "Title of your work",
    "section": "A Simple Example",
    "text": "A Simple Example\nThis is a Quarto document in which we can cite the bsvars package by Woźniak (2022). Look for more info at package CRAN website.\nSimply load the package by running\n\n\n\nThe code below performs simple computations for sampling posterior draws of the impulse responses. The first line uploads the data from the package,another sets the seed for reproducible computations, and then the pipe |> is used to streamline the model specification, estimation including the first burn-in run to obtain convergence and finally, the computed impulse responses are saved in object irf.\n\ndata(us_fiscal_lsuw)\nset.seed(123)\nus_fiscal_lsuw |>\n  specify_bsvar$new(p = 1) |>\n  estimate(S = 1000, show_progress = FALSE) |> \n  estimate(S = 2000, show_progress = FALSE) |> \n  compute_impulse_responses(horizon = 20) -> irf\n\nThe code above is visible as the R chunk contains the setting #| echo: true.\nTable 1 reports the posterior means of the gross domestic product response to an unanticipated tax increase by 1 pp.\n\n\n\n\nTable 1: Impulse response of gdp to unanticipated tax increase by 1 pp. within two years\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nchange in gdp [%]\n0.199\n0.184\n0.17\n0.159\n0.148\n0.139\n0.13\n0.123\n0.116\n\n\n\n\n\n\nFigure 1 presents the same reaction over the horizon of five years.\n\n\n\n\n\nFigure 1: Impulse response of gdp to unanticipated tax increase by 1 pp. within five years"
  },
  {
    "objectID": "index.html#some-hints",
    "href": "index.html#some-hints",
    "title": "Title of your work",
    "section": "Some Hints",
    "text": "Some Hints\nHave a look at how to work with RStudio and GitHub at: How to use git and GitHub with R.\nThese are many different ways of how to work with references in RStudio: Preview Citations.\nTo make all the R code visible on the website change the settings in the preabmble of this document to:\nexecute:\n  echo: true"
  },
  {
    "objectID": "Macroeconometrics research proposal.html",
    "href": "Macroeconometrics research proposal.html",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "",
    "text": ">**Keywords.** svars, impulse responses, quarto, R, monetary policy"
  },
  {
    "objectID": "Macroeconometrics research proposal.html#objective-and-motivation",
    "href": "Macroeconometrics research proposal.html#objective-and-motivation",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "Objective and Motivation",
    "text": "Objective and Motivation\nThe goal of this research projectis to analyze the impact of unconditional government stipends on the U.S. economy, such as stimulus checks. To identify these effects I will analyze the effects of the stimulus checks and increased unemployment benefits issued by the US government in the wake of the COVID-19 pandemic.\nThis is an important topic as inequality rises direct government action may become increasingly necessary. Increased taxation on the rich and targeted government programs can only do so much to help those at the lower and middle ends of income. Especially as the groups in need of help become larger and their needs more diverse, the high administrative costs of judging who gets the benefits and what they are applicable to may become untenable. In this case, regular stimulus payments or higher unemployment benefits may become necessary to the continued economic health of the country. However, without knowing the effects of these methods, it is difficult to say whether or not they would do more harm than good."
  },
  {
    "objectID": "Macroeconometrics research proposal.html#data",
    "href": "Macroeconometrics research proposal.html#data",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "Data",
    "text": "Data\nFor the purpose of this analysis data the United States Federal Reserve will be utilized. This source uses government data and is considered highly reliable. A potential issue is the information available being limited by the frequency with which the government collects data. To counter this, the estimates will use a long history and quarterly data which is the most frequent commonly available data.\nThe core factors chosen, median income, gdp per capita, and unemployment, were chosen for widely being considered to be strong indicators of economic health. An estimated four lags are used as this is quarterly data so by incorporating four lags seasonal effects are adequately accounted for.\nReal GDP Per Capita, Quarterly: https://fred.stlouisfed.org/series/A939RX0Q048SBEA\nUnemployment Rate, Monthly: https://fred.stlouisfed.org/series/UNRATE\nMedian Usual Weekly Real Earnings: https://fred.stlouisfed.org/series/LES1252881600Q\nMedian income is chosen over average income as, due to income disparity in the united states, there are large distortions in the average compared to the median. As stimulus payments and increased unemployment benefits are likely to more significantly impact lower income individuals it was determined that the median income would prove more suitable\nGDP per capita is selected as it accounts for fluctuations in population over time instead of including effects that could be simple shifts in total population\nUnemployment is utilized at it is of paramount concern in either confirming or assuaging concerns that, with relatively less incentive to work, that a portion of the population will elect not to work.\n\n#load requisite packages\nlibrary(fredr)\n\nWarning: package 'fredr' was built under R version 4.2.3\n\nlibrary(zoo)\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n#set key for accessing FRED data\nfredr_set_key(\"f1d6de070cd07cd6028872a3bc573657\")\n\n#Load in Fred data\n#Real GFP Per Capita and Unemployment Rate and median earnings\nrgdpcap = fredr(series_id = \"A939RX0Q048SBEA\")\nurate = fredr(series_id = \"UNRATE\")\nmearn = fredr(series_id = \"LES1252881600Q\")\n\n\n#Change unemployment data to quarterly\n#this errors but the entire rest of the the urate changes don't work \n#without it, the numbers seem to all be correct after checking by hand\nqurate = urate %>%\n  group_by(date = format(as.yearqtr(date, \"%b-%Y\"), \"%YQ%q\")) %>%\n  summarise_all(mean)\n\nWarning: There were 301 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `series_id = (function (x, ...) ...`.\nℹ In group 1: `date = \"1948Q1\"`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 300 remaining warnings.\n\n#pull only needed values\nrgdpcap = subset(rgdpcap, select = c(date, value))\n\n#rename column names to be more easily interpreted\ncolnames(rgdpcap) = c(\"Date\", \"Real GDP Per Capita\")\n\n#Change date to quarters to keep consistent\nrgdpcap$Date = as.yearqtr(rgdpcap$Date,\n                      format = \"%Y-%m-%d\")\n\n#select only useful columns\nurate = subset(qurate, select = c(date, value))\n\n#rename column names to be more easily interpreted\ncolnames(urate) = c(\"Date\", \"Unemployment Rate %\")\n\n#pull only needed values\nmearn = subset(mearn, select = c(date, value))\n\n#rename column names to be more easily interpreted\ncolnames(mearn) = c(\"Date\", \"Weekly Earnings\")\n\n#Change date to quarters to keep consistent\nmearn$Date = as.yearqtr(mearn$Date,\n                          format = \"%Y-%m-%d\")\n\n#problem\n#RGDP and income are in different chained values\n#solution, divide both by the relative CPI value to readjust to same real dollar\n#average CPI in 2012 is 229.594\n#average CPI in 2020 is 258.811\n#average CPI in 1982-1984 dollars is 100\n\n#formula to use, (CPI in new chain year * real value)/CPI in old chain year\n#adjust both to 2020 dollars\n\nrgdpcap$`Real GDP Per Capita` = (rgdpcap$`Real GDP Per Capita`*258.811)/229.594\n\nmearn$`Weekly Earnings` = (mearn$`Weekly Earnings`*258.811)/100\n\n#now that those values are about correct\n#equalize length of data\n#keep only values past the date of the newest data series\n#keep only values before 2023 as mearn doesn't have 2023 data\nurate = urate %>% filter(urate$Date >= '1979 Q1')\nurate = urate %>% filter(urate$Date < '2023 Q1')\n\n#adjust rgdpcap\nrgdpcap = rgdpcap %>% filter(rgdpcap$Date >= '1979 Q1')\n\n#values are all now equalized\n\n#urate date neded up as not a date so fix that\nurate$Date = as.yearqtr(urate$Date, format = \"%YQ%q\")\n\n#visually demonstrate values\nplot(urate$Date , urate$`Unemployment Rate %`, type = \"l\", \n     main = \"US Unemployment Rate Over Time\", xlab = \"Year/Quarter\", \n     ylab = \"Unemployment %\", col = \"blue\")\n\n\n\nplot(rgdpcap$Date , rgdpcap$`Real GDP Per Capita` , type = \"l\", \n     main = \"US Real GDP Per Capita (2020 Dollars) \", xlab = \"Year/Quarter\", \n     ylab = \"Real GDP in $\", col = \"blue\")\n\n\n\nplot(mearn$Date , mearn$`Weekly Earnings`, type = \"l\", \n     main = \"US Median Weekly Earnings (2020 Dollars\", xlab = \"Year/Quarter\", \n     ylab = \"Weekly Earnings in $\", col = \"blue\")"
  },
  {
    "objectID": "Macroeconometrics research proposal.html#model-and-hypothesis",
    "href": "Macroeconometrics research proposal.html#model-and-hypothesis",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "Model and Hypothesis",
    "text": "Model and Hypothesis\nThe model utilized will be a trivariate SVAR with the following specification\ninct + α1,2gdpt + α1,3uemt = β1,0 + β1,1inct-1 + β1,2gdpt-1 + β1,3uemt-1 + … + β1,10inct-4 + β1,11gdpt-4 + β1,12uemt-4 + μinct\nα2,1inct + gdpt + α2,3uemt = β2,0 + β2,1inct-1 + β2,2gdpt-1 + β2,3uemt-1 + … + β2,10inct-4 + β2,11gdpt-4 + β2,12uemt-4 + μgdpt\nα3,1inct + α3,2gdpt + uemt = β3,0 + β3,1inct-1 + β3,2gdpt-1 + β3,3uemt-1 + … + β3,10inct-4 + β3,11gdpt-4 + β3,12uemt-4 + μuemt\nWhere inc is median income, gdp is GDP per capita, uem is unemployment\nα terms are present to indicate present relationships between the variables\nβ1,0 β2,0 β3,0 are each intercept terms\nall other β terms are a multiplier on the value of prior period values on the present\n… indicates that the variables in question follow a repeating pattern, in this case lags from 1 to 4 periods\nμ is an error term\nt subscripts indicate the time period relative to the present. E.g. t-1 indicates the value of the variable one period in the past.\nI would also like to in my final report include a control variable for COVID as COVID doubtless impacted the economy in a way unrelated to any other changes.\nThis model goes to four lags as the data is quarterly and this ensures that seasonality effects are removed. The model will serve to identify the effects of shocks on the US economy stemming from stimulus payments as stimulus payments may be treated as one period shocks to income and the impulse response function will tell us the effects that such payments have on the economy.\nEstimating the effects of increased unemployment benefits is more difficult but can be achieved by treating these as a wage floor as presumably if one would be paid less than unemployment benefits one will choose not to work except as necessary to maintain benefits. Thus median income can be replaced with minimum wage in the above calculation and high levels of unemployment benefits treated as an effective increase in the minimum wage. As an alternative, the effects of a universal basic income program may be estimated as a permanent increase in income to all members of society regardless of their employment status.\nThese are all relevant to the economic situation in the United States going forward. A combination of stagnating wages, low minimum wage, and increasing income inequality threatens to force more forceful government action to avert economic crisis stemming from a lower-class which no longer lives at a subsistence level. While this state has not yet been reached it is a looming threat which must be addressed. The number of government programs to help low income individuals is immense but oftentimes much time and energy is spent ensuring that the “undeserving” are not given these benefits which can lead to those in need being rejected or ending up in worse circumstances due to long delays in receiving assistance. All of the proposed methods due to their weak targeting requirements would provide relief more rapidly and potentially aid in economic growth more than programs targeted at covering expenses related to a specific aspect of life."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "index.html#objective-and-motivation",
    "href": "index.html#objective-and-motivation",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "Objective and Motivation",
    "text": "Objective and Motivation\nThe goal of this research projectis to analyze the impact of unconditional government stipends on the U.S. economy, such as stimulus checks. To identify these effects I will analyze the effects of the stimulus checks and increased unemployment benefits issued by the US government in the wake of the COVID-19 pandemic.\nThis is an important topic as inequality rises direct government action may become increasingly necessary. Increased taxation on the rich and targeted government programs can only do so much to help those at the lower and middle ends of income. Especially as the groups in need of help become larger and their needs more diverse, the high administrative costs of judging who gets the benefits and what they are applicable to may become untenable. In this case, regular stimulus payments or higher unemployment benefits may become necessary to the continued economic health of the country. However, without knowing the effects of these methods, it is difficult to say whether or not they would do more harm than good."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "Data",
    "text": "Data\nFor the purpose of this analysis data the United States Federal Reserve will be utilized. This source uses government data and is considered highly reliable. A potential issue is the information available being limited by the frequency with which the government collects data. To counter this, the estimates will use a long history and quarterly data which is the most frequent commonly available data.\nThe core factors chosen, median income, gdp per capita, and unemployment, were chosen for widely being considered to be strong indicators of economic health. An estimated four lags are used as this is quarterly data so by incorporating four lags seasonal effects are adequately accounted for.\nReal GDP Per Capita, Quarterly: https://fred.stlouisfed.org/series/A939RX0Q048SBEA\nUnemployment Rate, Monthly: https://fred.stlouisfed.org/series/UNRATE\nMedian Usual Weekly Real Earnings: https://fred.stlouisfed.org/series/LES1252881600Q\nMedian income is chosen over average income as, due to income disparity in the united states, there are large distortions in the average compared to the median. As stimulus payments and increased unemployment benefits are likely to more significantly impact lower income individuals it was determined that the median income would prove more suitable\nGDP per capita is selected as it accounts for fluctuations in population over time instead of including effects that could be simple shifts in total population\nUnemployment is utilized at it is of paramount concern in either confirming or assuaging concerns that, with relatively less incentive to work, that a portion of the population will elect not to work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  as.vector(urate$`Unemployment Rate %`)\nDickey-Fuller = -3.0594, Lag order = 4, p-value = 0.1337\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  as.vector(mearn$`Weekly Earnings`)\nDickey-Fuller = -2.8796, Lag order = 4, p-value = 0.2087\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  as.vector(rgdpcap$`Real GDP Per Capita`)\nDickey-Fuller = -2.4063, Lag order = 4, p-value = 0.4065\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  as.vector(CPI$CPI)\nDickey-Fuller = -1.8612, Lag order = 4, p-value = 0.6344\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  as.vector(govex$`GovSpend in Billions`)\nDickey-Fuller = -2.7282, Lag order = 4, p-value = 0.2706\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  as.vector(M2$`M2 Money Supply`)\nDickey-Fuller = -1.4598, Lag order = 4, p-value = 0.8033\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(as.vector(urate$`Unemployment Rate %`), k = 3)\nDickey-Fuller = -5.5357, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(as.vector(mearn$`Weekly Earnings`), k = 3)\nDickey-Fuller = -5.5319, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(as.vector(rgdpcap$`Real GDP Per Capita`), k = 3)\nDickey-Fuller = -6.0544, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(as.vector(CPI$CPI), k = 3)\nDickey-Fuller = -2.6484, Lag order = 6, p-value = 0.3028\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(as.vector(govex$`GovSpend in Billions`), k = 3)\nDickey-Fuller = -4.3024, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(as.vector(M2$`M2 Money Supply`), k = 3)\nDickey-Fuller = -5.8067, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary"
  },
  {
    "objectID": "index.html#model-and-hypothesis",
    "href": "index.html#model-and-hypothesis",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "Model and Hypothesis",
    "text": "Model and Hypothesis\nThe model utilized will be a trivariate SVAR with the following specification\ninct + α1,2gdpt + α1,3uemt = β1,0 + β1,1inct-1 + β1,2gdpt-1 + β1,3uemt-1 + … + β1,10inct-4 + β1,11gdpt-4 + β1,12uemt-4 + μinct\nα2,1inct + gdpt + α2,3uemt = β2,0 + β2,1inct-1 + β2,2gdpt-1 + β2,3uemt-1 + … + β2,10inct-4 + β2,11gdpt-4 + β2,12uemt-4 + μgdpt\nα3,1inct + α3,2gdpt + uemt = β3,0 + β3,1inct-1 + β3,2gdpt-1 + β3,3uemt-1 + … + β3,10inct-4 + β3,11gdpt-4 + β3,12uemt-4 + μuemt\nWhere inc is median income, gdp is GDP per capita, uem is unemployment\nα terms are present to indicate present relationships between the variables\nβ1,0 β2,0 β3,0 are each intercept terms\nall other β terms are a multiplier on the value of prior period values on the present\n… indicates that the variables in question follow a repeating pattern, in this case lags from 1 to 4 periods\nμ is an error term\nt subscripts indicate the time period relative to the present. E.g. t-1 indicates the value of the variable one period in the past.\nI would also like to in my final report include a control variable for COVID as COVID doubtless impacted the economy in a way unrelated to any other changes.\nThis model goes to four lags as the data is quarterly and this ensures that seasonality effects are removed. The model will serve to identify the effects of shocks on the US economy stemming from stimulus payments as stimulus payments may be treated as one period shocks to income and the impulse response function will tell us the effects that such payments have on the economy.\nEstimating the effects of increased unemployment benefits is more difficult but can be achieved by treating these as a wage floor as presumably if one would be paid less than unemployment benefits one will choose not to work except as necessary to maintain benefits. Thus median income can be replaced with minimum wage in the above calculation and high levels of unemployment benefits treated as an effective increase in the minimum wage. As an alternative, the effects of a universal basic income program may be estimated as a permanent increase in income to all members of society regardless of their employment status.\nThese are all relevant to the economic situation in the United States going forward. A combination of stagnating wages, low minimum wage, and increasing income inequality threatens to force more forceful government action to avert economic crisis stemming from a lower-class which no longer lives at a subsistence level. While this state has not yet been reached it is a looming threat which must be addressed. The number of government programs to help low income individuals is immense but oftentimes much time and energy is spent ensuring that the “undeserving” are not given these benefits which can lead to those in need being rejected or ending up in worse circumstances due to long delays in receiving assistance. All of the proposed methods due to their weak targeting requirements would provide relief more rapidly and potentially aid in economic growth more than programs targeted at covering expenses related to a specific aspect of life.\n\n#Attempt at microcosm programming for the sake of later inclusion\n#in the overall model\n\n#first need to identify st1, st2, st3\n#cannot take as given\n#need to calculate additional terms\n#Beta is given by A.bar\n\n#Does gamma come in anywhere under a different name?\n#Lecture slides use S and ν for IW values as well as M, Σ, P\n#Paper uses ψ and d for IW values as well as b, Σ, Ω\n\n#Directions to derive St seem unclear\n#Theta is (s1, s2, s3, rho)\n#is it just p(gamma, theta|y)\n#But how is p(y|gamma, theta) derived?\n#IT seems to depend on already having st\n#Does it mean the prior of st? \n\n#set new yt based on given st\n#curent y lacks date\n#rebind date to verify\n#There must be a better way to select the appropriate dates\n#I just don't know offhand\n\ninstall.packages('MASS')\nlibrary(MASS)\n\ninstall.packages('distributionsrd')\nlibrary(distributionsrd)\n\n#set up for Metropolis-Hastings\nS = 1000\n\n#create c and w for covariance\nc = 0.0001\nW = diag(4)\n\n#create empty variables for accepted and itteration count\naccept = 0\nitter = 1\n\nomegaS = rep(0, 4)\nomegaco = c*W\n\n#is s just equal to omegastar\n\n#write separate function for posterior kernal\n#throw into for loop below for pomega\n\n#write function for evaluating st\n#generate theta vector from candidate generating density\n#use theta vector to generate st\n\n\n#run this later using an artificial s\n\n#Create function for generating p(y|Omega)\n\n#create function for generating tilde values\n#dataset, variance shift, lags\n\n#This is verified to work up until xtilde\n#must manually reset t for some reason\ntildecalc = function(y,s,p){\n  #generate ytilde\n  \n  #get length of y\n  obs = nrow(y)\n  \n  #Initialize matrix with all attributes of y to overwrite\n  ytilde = y\n  \n  #set up for loop to recalculate each row of y\n  #I am doing this because I do not recall if division of matrix by vector works\n  #so better to do a manual method I know works\n  for (i in 1:obs) {\n    \n    ytilde[i,]=y[i,]/s[i]\n    \n  } \n  #save new ytilde for future, keep old one to calculate xtilde\n  ytilde2 = ytilde[5:176,]\n  \n  #generate xtilde\n  #create new length variable\n  obsn = obs - p\n  \n  #initialize empty matrices\n  \n  xtildet = matrix(nrow = ncol(y),ncol = p+1)\n  \n  #xtilde is an array given that it is each set of xtildet\n  xtilde = array(dim = c(ncol(y),p+1,obsn))\n  \n  xtildet[,1] = 1\n  #create t variable to track current timeperiod\n  t = 1+p\n  \n  #populate xtilde\n  for (i in 1:obsn){\n    xtildet[,1] = 1\n    \n    \n    #update xtildet for new period based on number of lags\n    for (j in 1:p){\n      #Use loop count to select aprropriate rows of y\n      #lcount + p - j ensures rows populate in proper order\n      \n      #this returns out of bounds error because yes\n      #it seems like it doesn't properly reset t before checkingthis\n      #The code works if I manually reset t then run the loop\n      xtildet[,j+1] = t(y[t-j,])\n    }\n    \n    \n    #perform math to make xtildet be correct\n    xtildet = xtildet/s[p+i]\n    #now fill in xtilde with the val\n    \n    xtilde[,,i] = xtildet\n    #increment current time\n    t = t + 1  \n  }\n  #is transposing an array even neceasry? It's an array\n  t = 1+p\n}\n\n\n\n#try for loop version\nfor (i in 1:S) {\n  #prior density\n  rhostar = dnorm(0,0.5)\n  s = dpareto()\n  \n  \n  #candidate generating density\n  omegastar = mvrnorm(1, omegaS, omegaco)\n  \n  \n  pomega = det(diag(s)) ^ /2 * #p(omega) Ask prof about this, I can't quite read handwriting\n  u = runif(0,1)\n  alphaS = min(pomega,1)\n  \n  \n  if (u < alpahaS){\n    omegaS = omegastar\n    #save kernal of omegaS\n    accept = accept + 1\n  } else {\n      omegaS = omegaS\n      #save kernal of omegaS\n  }\n  itteration = itteration + 1\n}\n  \n#calculate rejection rate\nrrate = 1 - accept/itteration\n\n#do I actually need this or can I hand-calc it above?\ncoda::rejectionRate()\n\n\n\n\n\n\nrho = rnorm(mean = 0, sd = 0.5)\n\ns = rep(1,160)\n\n#initialize s's\ns0 = 1\ns1 = 2\ns2 = 3\n#initialize empty rho vector\nrhos = as.matrix(rep(0,9))\nrhos[1,]\n#for loop to find all variances\nfor (i in 1:9){\n  #can chage j-2 to just i since i is j-2 already\n  #itterates through each time period to calculate new value based on s2 and rho\n  rhos[i,] = 1 + (s2-1) * rho ^ i\n}\n\n#extend this to include all future values determined by rho\ns = c(s, s0, s1, s2, rhos)\nsdia = diag(s)\n#Check back later\n#may not need to actually use these.\ny = cbind(urate[,1], y)\nyt1 = y[165,2:7]/s1\nyt2 = y[166,2:7]/s2\nyt3 = y[167,2:7]/s3\n\n#reinsert these rows back into the original y matrix\n#only separate for verification purposes here to ensure math was done correctly\n#directly give correct output in final code\n\n\n#should be able to construct a for loop of some kind\n#try to do that later\n\n\n#generate Xt\n\n\n#set new Xt based on given st\n\n#regenerate Beta and Sigma using MLE of new values\nA.hat       = solve(t(X)%*%sdia%*%X)%*%t(X)%*%sdia%*%Y\nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n\n#This is all just given and can be reused infinitely\nV.bar.inv   = t(X)%*%diag(1/s)%*%X + diag(1/diag(V.prior))\nV.bar       = solve(V.bar.inv)\nA.bar       = V.bar%*%(t(X)%*%diag(1/s)%*%Y + diag(1/diag(V.prior))%*%A.prior)\nnu.bar      = nrow(Y) + nu.prior\nS.bar       = S.prior + t(Y)%*%diag(1/s)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\nS.bar.inv   = solve(S.bar)\n#I hope this code works\n\n#equalize length of data\n#keep only values past the date of the newest data series\n#keep only values before 2023\nmearn = mearn %>% filter(mearn$Date >= '1979 Q1')\nmearn = mearn %>% filter(mearn$Date < '2023 Q1')\n\nrgdpcap = rgdpcap %>% filter(rgdpcap$Date >= '1979 Q1')\nrgdpcap = rgdpcap %>% filter(rgdpcap$Date < '2023 Q1')\n\n\n#CPI doesn't work, but if I change name and change back it works\nCPI2 = CPI\n\nCPI2 = CPI2 %>% filter(CPI2$Date >= '1979 Q1')\nCPI = CPI2 %>% filter(CPI2$Date < '2023 Q1')\n\ngovex = govex %>% filter(govex$Date >= '1979 Q1')\ngovex = govex %>% filter(govex$Date < '2023 Q1')\n\nM2 = M2 %>% filter(M2$Date >= '1979 Q1')\nM2 = M2 %>% filter(M2$Date < '2023 Q1')\n\n#set up y matrix\n#doing it like this so I can toggle stuff off and on\ny = cbind(urate[,2], mearn[,2])\ny = cbind(y, rgdpcap[,2])\ny = cbind(y, CPI[,2])\ny = cbind(y, govex[,2])\ny = cbind(y, M2[,2])\n\n\n#Need to set y as matrix or X becomes wrong object type for future calcs\ny = as.matrix(y)\n\n#To ensure consistency later in setting Sign Restrictions\n#y has variables in the following order\n#Unemployment Rate\n#Mean Earnings\n#Real GDP Per Capita\n#Consumer Price Index\n#Real Government Consumption Expenditure\n#M2 Money Supply\n#Change the above as needed to ensure all sign restrictions are on the left\n\n\n#create VAR model?\n#VAR(4) for 1 year of lags\n\n# setup\n############################################################\n\n#number of variables\nN       = 6\n#number of lags\np       = 4\n#number of draws\nS       = 50000\n#not the foggiest\nh       = 8\n\n# create Y and X\n############################################################  \n\nY       = ts(y[5:176,])\nX       = matrix(1,nrow(Y),1)\nfor (i in 1:p){\n  X     = cbind(X,y[5:176-i,])\n}\n\nt0          = proc.time() # read processor time\n\n\n\n# MLE\n############################################################\nA.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n\n#kappa 1 is 1 if non-stationary\n#smaller than 1 if stationary\nkappa.1     = 1^2\n#k2 = 100 is given for Minnesota Prior\nkappa.2     = 100\n#kappa 3 is 1 just given the fact that it determines A.prior\n#which invovles the identity matrix\nkappa.3     = 1\nA.prior     = matrix(0,nrow(A.hat),ncol(A.hat))\n#bit in brackets is just 2:N+1\nA.prior[2:7,] = kappa.3*diag(N)\nV.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\nS.prior     = diag(diag(Sigma.hat))\nnu.prior    = N+1\n\n# normal-inverse Wishard posterior parameters\n############################################################\n\n#This is all just given and can be reused infinitely\nV.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))\nV.bar       = solve(V.bar.inv)\nA.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)\nnu.bar      = nrow(Y) + nu.prior\nS.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\nS.bar.inv   = solve(S.bar)\n\n#Ok so all of that above should get me all the posterior distribution\n#Now I need to actually do anything whatsoever with sign restrctions\n\n# Estimating models with sign restrictions: example\n# Use Algorithm 2\n############################################################\nrm(list=ls())\nset.seed(123456)\n#column by column\nsign.restrictions = c(-1,-1,1,-1,-1,1)\n#this is fine just regardless of how many there are\nR1            = diag(sign.restrictions)\n#Check how to derive\n#I think this is just the posterior\nA = A.bar\n#check how to derive\n#pretty sure this is also just a posterior\nSigma         = S.bar\n#these should be a given I *think*\n#error with B0.tilde\n\nB0.tilde      = t(solve(chol(Sigma)))\nB1.tilde      = B0.tilde%*%A\nIR.0.tilde    = solve(B0.tilde)\nIR.1.tilde    = solve(B0.tilde)%*%B1.tilde%*%solve(B0.tilde)\n\n#This just repeats until I get one where sign restrictions hold\n#Assuming the above is all set up properly this shoudl require no more input\n#some of this might require a touch of editing\n#E,g, X may be based on some given parameters instead of being arbitrary\nsign.restrictions.do.not.hold = TRUE\ni=1\nwhile (sign.restrictions.do.not.hold){\n  X           = matrix(rnorm(9),3,3)\n  QR          = qr(X, tol = 1e-10)\n  Q           = qr.Q(QR,complete=TRUE)\n  R           = qr.R(QR,complete=TRUE)\n  Q           = t(Q %*% diag(sign(diag(R))))\n  B0          = Q%*%B0.tilde\n  B1          = Q%*%B1.tilde\n  B0.inv      = solve(B0)\n  check       = prod(R1 %*% rbind(B0.inv,B0.inv%*%B1%*%B0.inv) %*% diag(3)[,1] > 0)\n  if (check==1){sign.restrictions.do.not.hold=FALSE}\n  i=i+1\n}\ni\nQ\nB0\nB1\nIR.0        = B0.inv\nIR.1        = B0.inv%*%B1%*%B0.inv\nIR.0\nIR.1\nR1 %*% rbind(IR.0,IR.1) %*% diag(3)[,1]"
  },
  {
    "objectID": "index.html#preliminary-data-analysis",
    "href": "index.html#preliminary-data-analysis",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "Preliminary data analysis",
    "text": "Preliminary data analysis\n\n\n# A tibble: 176 × 1\n   `Unemployment Rate %`\n                   <dbl>\n 1                  5.87\n 2                  5.7 \n 3                  5.87\n 4                  5.97\n 5                  6.3 \n 6                  7.33\n 7                  7.67\n 8                  7.4 \n 9                  7.43\n10                  7.4 \n# … with 166 more rows\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  as.vector(urate$`Unemployment Rate %`)\nDickey-Fuller = -3.0594, Lag order = 4, p-value = 0.1337\nalternative hypothesis: stationary\n\n\nWarning in tseries::adf.test(diff(as.vector(urate$`Unemployment Rate %`)), :\np-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(as.vector(urate$`Unemployment Rate %`))\nDickey-Fuller = -6.6496, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary"
  }
]