---
title: "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy"
author: "Corwin Getty"

execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.**
>
> **Keywords.** svars, impulse responses, quarto, R, monetary policy

# 

## Objective and Motivation

The goal of this research projectis to analyze the impact of unconditional government stipends on the U.S. economy, such as stimulus checks. To identify these effects I will analyze the effects of the stimulus checks and increased unemployment benefits issued by the US government in the wake of the COVID-19 pandemic.

This is an important topic as inequality rises direct government action may become increasingly necessary. Increased taxation on the rich and targeted government programs can only do so much to help those at the lower and middle ends of income. Especially as the groups in need of help become larger and their needs more diverse, the high administrative costs of judging who gets the benefits and what they are applicable to may become untenable. In this case, regular stimulus payments or higher unemployment benefits may become necessary to the continued economic health of the country. However, without knowing the effects of these methods, it is difficult to say whether or not they would do more harm than good.

## Data

For the purpose of this analysis data the United States Federal Reserve will be utilized. This source uses government data and is considered highly reliable. A potential issue is the information available being limited by the frequency with which the government collects data. To counter this, the estimates will use a long history and quarterly data which is the most frequent commonly available data.

The core factors chosen, median income, gdp per capita, and unemployment, were chosen for widely being considered to be strong indicators of economic health. An estimated four lags are used as this is quarterly data so by incorporating four lags seasonal effects are adequately accounted for.

Real GDP Per Capita, Quarterly: <https://fred.stlouisfed.org/series/A939RX0Q048SBEA>

Unemployment Rate, Monthly: <https://fred.stlouisfed.org/series/UNRATE>

Median Usual Weekly Real Earnings: <https://fred.stlouisfed.org/series/LES1252881600Q>

Consumer Price Index, Less Food and Energy: <https://fred.stlouisfed.org/series/CPILFESL>

Real Government Consumption Expenditures: <https://fred.stlouisfed.org/series/A955RX1Q020SBEA>

M2 Real Money Supply: <https://fred.stlouisfed.org/series/M2REAL>

Median income is chosen over average income as, due to income disparity in the united states, there are large distortions in the average compared to the median. As stimulus payments and increased unemployment benefits are likely to more significantly impact lower income individuals it was determined that the median income would prove more suitable

GDP per capita is selected as it accounts for fluctuations in population over time instead of including effects that could be simple shifts in total population

Unemployment is utilized at it is of paramount concern in either confirming or assuaging concerns that, with relatively less incentive to work, that a portion of the population will elect not to work.

CPI will reflect the changes in price level as a result of subsidies. Prices of food and energy are ignored as there are a large number of confounding factors which may distort data but are usually short term shocks as opposed to more systemic changes. For example, eggs fluctuated wildly in price in the United States for several months in early 2023 due solely to the conditions of the egg market and not due to any outside effects. For more durable goods it is expected that the changes in price level will be due largely due to more systemic shifts than short-term changes.

Real government consumption will certainly rise, at least short term, when income rises as a result of the increase in income being due to government subsidy. Of interest is signs of potential decrease in government expenditure in other sectors due to rising incomes, or if no such signs appear.

M2 money supply is included as contraction or expansion of the money supply can serve as an indicator for several other variables such as investment, interest rates, and spending.

All vlaues except unemployment rate are taken as first differences to make them a stationary series. Unemployment rate is already stationary and as such does not require transformation. Both the nonstationary and the stationary series will be shown.

```{r}
#| code-fold: true
#| code-summary: "Show code"
#| message: false
#| warning: false


#load requisite packages
library(fredr)
library(zoo)
library(dplyr)
library(coda)
library(HDInterval)
library(MASS)
library(distributionsrd)
library(arrayhelpers)
#set key for accessing FRED data
fredr_set_key("f1d6de070cd07cd6028872a3bc573657")

#Load in Fred data
#Real GFP Per Capita and Unemployment Rate and median earnings
rgdpcap = fredr(series_id = "A939RX0Q048SBEA")
urate = fredr(series_id = "UNRATE")
mearn = fredr(series_id = "LES1252881600Q")


#keep only unemployment rate and date to avoid errors
urate = subset(urate, select = c(date, value))

#Change unemployment data to quarterly
urate = urate %>%
  group_by(date = format(as.yearqtr(date, "%b-%Y"), "%YQ%q")) %>%
  summarise_all(mean)

#pull only needed values
rgdpcap = subset(rgdpcap, select = c(date, value))

#rename column names to be more easily interpreted
colnames(rgdpcap) = c("Date", "Real GDP Per Capita")

#Change date to quarters to keep consistent
rgdpcap$Date = as.yearqtr(rgdpcap$Date,
                      format = "%Y-%m-%d")


#rename column names to be more easily interpreted
colnames(urate) = c("Date", "Unemployment Rate %")

#pull only needed values
mearn = subset(mearn, select = c(date, value))

#rename column names to be more easily interpreted
colnames(mearn) = c("Date", "Weekly Earnings")

#Change date to quarters to keep consistent
mearn$Date = as.yearqtr(mearn$Date,
                          format = "%Y-%m-%d")

#problem
#RGDP and income are in different chained values
#solution, divide both by the relative CPI value to readjust to same real dollar
#average CPI in 2012 is 229.594
#average CPI in 2020 is 258.811
#average CPI in 1982-1984 dollars is 100

#formula to use, (CPI in new chain year * real value)/CPI in old chain year
#adjust both to 2020 dollars

rgdpcap$`Real GDP Per Capita` = (rgdpcap$`Real GDP Per Capita`*258.811)/229.594

mearn$`Weekly Earnings` = (mearn$`Weekly Earnings`*258.811)/100

#now that those values are about correct
#equalize length of data
#keep only values past the date of the newest data series
#keep only values before 2023 as mearn doesn't have 2023 data
urate = urate %>% filter(urate$Date >= '1979 Q1')
urate = urate %>% filter(urate$Date < '2023 Q1')

#adjust rgdpcap
rgdpcap = rgdpcap %>% filter(rgdpcap$Date >= '1979 Q1')

#values are all now equalized

#urate date neded up as not a date so fix that
urate$Date = as.yearqtr(urate$Date, format = "%YQ%q")

#load in CPI, as Inflation is a key target of the FED

CPI = fredr(series_id = "CPILFESL")

#select only needed data

CPI = subset(CPI, select = c(date, value))

#Adjust CPI to quarterly

CPI = CPI %>%
  group_by(date = format(as.yearqtr(date, "%b-%Y"), "%YQ%q")) %>%
  summarise_all(mean)

#rename column names to be more easily interpreted
colnames(CPI) = c("Date", "CPI")


CPI$Date = as.yearqtr(CPI$Date, format = "%YQ%q")



#should maybe include M2 money supply as a possible vector
#issue with M2 money suppl defiintion being changed
#large upswing in 2020 that is not due to real changes
#Also truncates data 2 years later
#large information loss given already small number of observations
#do not include at present 

#load in government consumption spending

govex = fredr(series_id = "A955RX1Q020SBEA")

#select only needed data

govex = subset(govex, select = c(date, value))

#Re-chain to 2020 dollars
govex$value = (govex$value*258.811)/229.594


#rename column names to be more easily interpreted
colnames(govex) = c("Date", "GovSpend in Billions")


govex$Date = as.yearqtr(govex$Date, format = "%YQ%q")




#load in real M2

M2 = fredr(series_id = "M2REAL")

#select only needed data

M2 = subset(M2, select = c(date, value))

#Re-chain to 2020 dollars
M2$value = (M2$value*258.811)/100

#adjust to be quarterly

M2 = M2 %>%
  group_by(date = format(as.yearqtr(date, "%b-%Y"), "%YQ%q")) %>%
  summarise_all(mean)

#rename column names to be more easily interpreted
colnames(M2) = c("Date", "M2 Money Supply")


M2$Date = as.yearqtr(M2$Date, format = "%YQ%q")



#visually demonstrate values
plot(urate$Date , urate$`Unemployment Rate %`, type = "l", lwd = 3,
     main = "US Unemployment Rate Over Time", xlab = "Year/Quarter", 
     ylab = "Unemployment %", col = "blue")

plot(mearn$Date , mearn$`Weekly Earnings`, type = "l", lwd = 3,
     main = "US Median Weekly Earnings (2020 Dollars)", xlab = "Year/Quarter", 
     ylab = "Weekly Earnings in $", col = "blue")

plot(rgdpcap$Date , rgdpcap$`Real GDP Per Capita` , type = "l", lwd = 3,
     main = "US Real GDP Per Capita (2020 Dollars)", xlab = "Year/Quarter", 
     ylab = "Real GDP in $", col = "blue")

plot(CPI$Date , CPI$CPI, type = "l", lwd = 3,
     main = "US CPI Over Time (Excluding Food and Energy)", xlab = "Year/Quarter", 
     ylab = "CPI (1983 Base)", col = "blue")

plot(govex$Date , govex$`GovSpend in Billions`, type = "l", lwd = 3,
     main = "US Governement Consumption Over Time", xlab = "Year/Quarter", 
     ylab = "Billions of 2020 dollars", col = "blue")

plot(M2$Date , M2$`M2 Money Supply`, type = "l", lwd = 3,
     main = "US M2 Money Supply Over Time", xlab = "Year/Quarter", 
     ylab = "Billions of 2020 dollars", col = "blue")

#plot autocorrelation functions

acf(urate[,2])
acf(mearn[,2])
acf(rgdpcap[,2])
acf(CPI[,2])
acf(govex[,2])
acf(M2[,2])


#plot PACF
pacf(urate[,2])
pacf(mearn[,2])
pacf(rgdpcap[,2])
pacf(CPI[,2])
pacf(govex[,2])
pacf(M2[,2])

#ADF testing
tseries::adf.test(as.vector(urate$`Unemployment Rate %`), k = 4)
tseries::adf.test(as.vector(mearn$`Weekly Earnings`), k = 4)
tseries::adf.test(as.vector(rgdpcap$`Real GDP Per Capita`), k = 4)
tseries::adf.test(as.vector(CPI$CPI), k = 4)
tseries::adf.test(as.vector(govex$`GovSpend in Billions`), k = 4)
tseries::adf.test(as.vector(M2$`M2 Money Supply`), k = 4)

#For all reject unit root stationarity

tseries::adf.test(diff(as.vector(urate$`Unemployment Rate %`), k = 3))
tseries::adf.test(diff(as.vector(mearn$`Weekly Earnings`), k = 3))
tseries::adf.test(diff(as.vector(rgdpcap$`Real GDP Per Capita`), k = 3))
tseries::adf.test(diff(as.vector(CPI$CPI), k = 3))
tseries::adf.test(diff(as.vector(govex$`GovSpend in Billions`), k = 3))
tseries::adf.test(diff(as.vector(M2$`M2 Money Supply`), k = 3))

```

First differences values are shown below.

```{r}
#Take first differences
#It's making me select an end point if I want to use not all of column 1
n = nrow(mearn)
mearn = cbind(mearn[2:n,1] ,diff(mearn$`Weekly Earnings`))
#colname since its renaming and being annoying
colnames(mearn) = c("Date", "Weekly Earnings")

n = nrow(rgdpcap)
rgdpcap = cbind(rgdpcap[2:n,1], diff(rgdpcap$`Real GDP Per Capita`))
colnames(rgdpcap) = c("Date", "Real GDP Per Capita")

n = nrow(CPI)
CPI = cbind(CPI[2:n,1],diff(CPI$CPI))
colnames(CPI) = c("Date", "CPI")

n = nrow(govex)
govex = cbind(govex[2:n,1],diff(govex$`GovSpend in Billions`))
colnames(govex) = c("Date", "GovSpend in Billions")

n = nrow(M2)
M2 = cbind(M2[2:n,1],diff(M2$`M2 Money Supply`))
colnames(M2) = c("Date", "M2 Money Supply")

#visually demonstrate values
plot(urate$Date, urate$`Unemployment Rate %`, type = "l", lwd = 3,
     main = "US Unemployment Rate Over Time", xlab = "Year/Quarter", 
     ylab = "Unemployment %", col = "blue")

plot(mearn$`Date` ,mearn$`Weekly Earnings` , type = "l", lwd = 3,
     main = "US Median Weekly Earnings (2020 Dollars) First Difference", xlab = "Year/Quarter", 
     ylab = "Weekly Earnings in $", col = "blue")

plot(rgdpcap$Date , rgdpcap$`Real GDP Per Capita` , type = "l", lwd = 3,
     main = "US Real GDP Per Capita (2020 Dollars) First Difference", xlab = "Year/Quarter", 
     ylab = "Real GDP in $", col = "blue")

plot(CPI$Date , CPI$CPI, type = "l", lwd = 3,
     main = "US CPI Over Time (Excluding Food and Energy) First Difference", xlab = "Year/Quarter", 
     ylab = "CPI (1983 Base)", col = "blue")

plot(govex$Date , govex$`GovSpend in Billions`, type = "l", lwd = 3,
     main = "US Governement Consumption Over Time First Difference", xlab = "Year/Quarter", 
     ylab = "Billions of 2020 dollars", col = "blue")

plot(M2$Date , M2$`M2 Money Supply`, type = "l", lwd = 3,
     main = "US M2 Money Supply Over Time First Difference", xlab = "Year/Quarter", 
     ylab = "Billions of 2020 dollars", col = "blue")



```

## Preliminary data analysis THE ABOVE NEEDS TO BE BROUGHT INTO THIS AND SPACE APPROPRIATELY

All of the variables of interest show a gradually decaying ACF and essentially no significant PACF. This is consistent with variables that have a strong dependence on prior observed values and no significant dependence on the error of the prior observations. This indicates that an AR model is indeed useful for predicting the behavior of these variables.

The ADF tests indicate that all of the processes are unit root non-stationary. This is consistent with expectations as all of the variables visually exhibit a strong upward trend. Unemployment rate being non-stationary is something of a surprise, but it has the smallest p-value at 0.1337. On visual inspection it can be seen that, barring the COVID shock, unemployment rate has been cyclical but downward trending.

As can be seen above, many of the series are reasonably static over time except for the beginning of the COVID-19 pandemic. This sudden, dramatic change can skew data, especially given the relatively small number of observations amplifying the effects of any outliers. As a result a simple SVAR may not return accurate results due to attributing too much weight to COVID shocks since SVAR models have difficulty accounting for exogenous shocks.

To compensate for this the variance spike during the early periods of COVID will need to be accounted for.


## Model and Hypothesis THIS SECTION IS NEXT TO BE UPDATED

The model utilized will be a six variables SVAR with the following specification

*inc~t~* + α~1,2~gdp~t~ + α~1,3~uem~t~ = β~1,0~ + β~1,1~inc~t-1~ + β~1,2~gdp~t-1~ + β~1,3~uem~t-1~ + ... + β~1,10~inc~t-4~ + β~1,11~gdp~t-4~ + β~1,12~uem~t-4~ + μ~inct~

*α~2,1~inc~t~* + gdp~t~ + α~2,3~uem~t~ = β~2,0~ + β~2,1~inc~t-1~ + β~2,2~gdp~t-1~ + β~2,3~uem~t-1~ + ... + β~2,10~inc~t-4~ + β~2,11~gdp~t-4~ + β~2,12~uem~t-4~ + μ~gdpt~

*α~3,1~inc~t~* + α~3,2~gdp~t~ + uem~t~ = β~3,0~ + β~3,1~inc~t-1~ + β~3,2~gdp~t-1~ + β~3,3~uem~t-1~ + ... + β~3,10~inc~t-4~ + β~3,11~gdp~t-4~ + β~3,12~uem~t-4~ + μ~uemt~

Where inc is median income, gdp is GDP per capita, uem is unemployment

α terms are present to indicate present relationships between the variables

β~1,0~ β~2,0~ β~3,0~ are each intercept terms

all other β terms are a multiplier on the value of prior period values on the present

... indicates that the variables in question follow a repeating pattern, in this case lags from 1 to 4 periods

μ is an error term

t subscripts indicate the time period relative to the present. E.g. t-1 indicates the value of the variable one period in the past.

I would also like to in my final report include a control variable for COVID as COVID doubtless impacted the economy in a way unrelated to any other changes.

This model goes to four lags as the data is quarterly and this ensures that seasonality effects are removed. The model will serve to identify the effects of shocks on the US economy stemming from stimulus payments as stimulus payments may be treated as one period shocks to income and the impulse response function will tell us the effects that such payments have on the economy.

Estimating the effects of increased unemployment benefits is more difficult but can be achieved by treating these as a wage floor as presumably if one would be paid less than unemployment benefits one will choose not to work except as necessary to maintain benefits. Thus median income can be replaced with minimum wage in the above calculation and high levels of unemployment benefits treated as an effective increase in the minimum wage. As an alternative, the effects of a universal basic income program may be estimated as a permanent increase in income to all members of society regardless of their employment status.

These are all relevant to the economic situation in the United States going forward. A combination of stagnating wages, low minimum wage, and increasing income inequality threatens to force more forceful government action to avert economic crisis stemming from a lower-class which no longer lives at a subsistence level. While this state has not yet been reached it is a looming threat which must be addressed. The number of government programs to help low income individuals is immense but oftentimes much time and energy is spent ensuring that the "undeserving" are not given these benefits which can lead to those in need being rejected or ending up in worse circumstances due to long delays in receiving assistance. All of the proposed methods due to their weak targeting requirements would provide relief more rapidly and potentially aid in economic growth more than programs targeted at covering expenses related to a specific aspect of life.

#THIS IS TEMPORARILY COMMENTED OUT, ADD ANOTHER ` BELOW TO RETURN TO CODE
``{r}

#Attempt at microcosm programming for the sake of later inclusion
#in the overall model

#first need to identify st1, st2, st3
#cannot take as given
#need to calculate additional terms
#Beta is given by A.bar

#Does gamma come in anywhere under a different name?
#Lecture slides use S and ν for IW values as well as M, Σ, P
#Paper uses ψ and d for IW values as well as b, Σ, Ω

#Directions to derive St seem unclear
#Theta is (s1, s2, s3, rho)
#is it just p(gamma, theta|y)
#But how is p(y|gamma, theta) derived?
#IT seems to depend on already having st
#Does it mean the prior of st? 

#set new yt based on given st
#curent y lacks date
#rebind date to verify
#There must be a better way to select the appropriate dates
#I just don't know offhand

#install.packages('MASS')
library(MASS)

#install.packages('distributionsrd')
library(distributionsrd)

#install.packages('arrayhelpers')
library(arrayhelpers)

#set up for Metropolis-Hastings
S = 1000

#create c and w for covariance
c = 0.0001
W = diag(4)
p=4
#create empty variables for accepted and iteration count
accept = 0
itter = 1

omegaS = rep(0, 4)
omegaco = c*W

#is s just equal to omegastar

#write separate function for posterior kernal
#throw into for loop below for pomega

#write function for evaluating st
#generate theta vector from candidate generating density
#use theta vector to generate st
tildecalc()

#run this later using an artificial s

#Create function for generating p(y|Omega)

#create function for generating tilde values
#dataset, variance shift, lags

#This is verified to work up until xtilde
#must manually reset t for some reason
tildecalc = function(){
  #generate ytilde
  
  #get length of y
  obs = nrow(y)
  
  #Initialize matrix with all attributes of y to overwrite
  ytilde = y
  
  #set up for loop to recalculate each row of y
  #I am doing this because I do not recall if division of matrix by vector works
  #so better to do a manual method I know works
  for (i in 1:obs) {
    
    ytilde[i,]=y[i,]/s[i]
    
  } 
  #save new ytilde for future, keep old one to calculate xtilde
  ytilde2 = ytilde[5:176,]
  
  #generate xtilde
  #create new length variable
  obsn = obs - p
  
  #initialize empty matrices
  
  xtildet = matrix(nrow = ncol(y),ncol = p+1)
  
  #xtilde is an array given that it is each set of xtildet
  xtilde = array(dim = c(ncol(y),p+1,obsn))
  
  xtildet[,1] = 1
  #create t variable to track current timeperiod
  t = 1+p
  
  #populate xtilde
  for (i in 1:obsn){
    xtildet[,1] = 1
    
    
    #update xtildet for new period based on number of lags
    for (j in 1:p){
      #Use loop count to select aprropriate rows of y
      #lcount + p - j ensures rows populate in proper order
      
      #this returns out of bounds error because yes
      #it seems like it doesn't properly reset t before checkingthis
      #The code works if I manually reset t then run the loop
      xtildet[,j+1] = t(y[t-j,])
    }
    
    
    #perform math to make xtildet be correct
    xtildet = xtildet/s[p+i]
    #now fill in xtilde with the val
    
    xtilde[,,i] = xtildet
    #increment current time
    t = t + 1  
  }
  #is transposing an array even necessary? It's an array
  t = 1+p


#Create a transpose of array xtilde
  xtildetrans = ta(xtilde)
  
  btildehat = (xtildetrans%*%xtilde + solve(omega)) %*% (xtildetrans%*%ytilde2 + solve(omega)%*%bhat)
  
  epsiltildehat = ytilde2 - xtilde%*%btildehat
}
#I need to find some 

#try for loop version
for (i in 1:S) {
  #prior density
  rhostar = dnorm(0,0.5)
  s = dpareto()
  
  
  #candidate generating density
  omegastar = mvrnorm(1, omegaS, omegaco)
  
  
  pomega = det(diag(s)) ^ /2 * #p(omega) Ask prof about this, I can't quite read handwriting
  u = runif(0,1)
  alphaS = min(pomega,1)
  
  
  if (u < alpahaS){
    omegaS = omegastar
    #save kernal of omegaS
    accept = accept + 1
  } else {
      omegaS = omegaS
      #save kernal of omegaS
  }
  itteration = itteration + 1
}
  
#calculate rejection rate
rrate = 1 - accept/itteration

#do I actually need this or can I hand-calc it above?
coda::rejectionRate()






rho = rnorm(mean = 0, sd = 0.5)

s = rep(1,160)

#initialize s's
s0 = 1
s1 = 2
s2 = 3
#initialize empty rho vector
rhos = as.matrix(rep(0,9))
rhos[1,]
#for loop to find all variances
for (i in 1:9){
  #can chage j-2 to just i since i is j-2 already
  #itterates through each time period to calculate new value based on s2 and rho
  rhos[i,] = 1 + (s2-1) * rho ^ i
}

#extend this to include all future values determined by rho
s = c(s, s0, s1, s2, rhos)
sdia = diag(s)
#Check back later
#may not need to actually use these.
y = cbind(urate[,1], y)
yt1 = y[165,2:7]/s1
yt2 = y[166,2:7]/s2
yt3 = y[167,2:7]/s3

#reinsert these rows back into the original y matrix
#only separate for verification purposes here to ensure math was done correctly
#directly give correct output in final code


#should be able to construct a for loop of some kind
#try to do that later


#generate Xt


#set new Xt based on given st

#regenerate Beta and Sigma using MLE of new values
A.hat       = solve(t(X)%*%sdia%*%X)%*%t(X)%*%sdia%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

#This is all just given and can be reused infinitely
V.bar.inv   = t(X)%*%diag(1/s)%*%X + diag(1/diag(V.prior))
V.bar       = solve(V.bar.inv)
A.bar       = V.bar%*%(t(X)%*%diag(1/s)%*%Y + diag(1/diag(V.prior))%*%A.prior)
nu.bar      = nrow(Y) + nu.prior
S.bar       = S.prior + t(Y)%*%diag(1/s)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
S.bar.inv   = solve(S.bar)
```

```{}
```{r}
#| code-fold: true
#| code-summary: "Show code"


#I hope this code works

#equalize length of data
#keep only values past the date of the newest data series
#keep only values before 2023
n = nrow(urate)
urate = urate[2:n,]

mearn = mearn %>% filter(mearn$Date >= '1979 Q2')
mearn = mearn %>% filter(mearn$Date < '2023 Q1')

rgdpcap = rgdpcap %>% filter(rgdpcap$Date >= '1979 Q2')
rgdpcap = rgdpcap %>% filter(rgdpcap$Date < '2023 Q1')


#CPI doesn't work, but if I change name and change back it works
CPI2 = CPI

CPI2 = CPI2 %>% filter(CPI2$Date >= '1979 Q2')
CPI = CPI2 %>% filter(CPI2$Date < '2023 Q1')

govex = govex %>% filter(govex$Date >= '1979 Q2')
govex = govex %>% filter(govex$Date < '2023 Q1')

M2 = M2 %>% filter(M2$Date >= '1979 Q2')
M2 = M2 %>% filter(M2$Date < '2023 Q1')

#set up y matrix
#doing it like this so I can toggle stuff off and on
y = cbind(mearn[,2], urate[,2])
y = cbind(y, rgdpcap[,2])
y = cbind(y, CPI[,2])
y = cbind(y, govex[,2])
y = cbind(y, M2[,2])


#Need to set y as matrix or X becomes wrong object type for future calcs
y = as.matrix(y)
#use n for anywhere I need the length of the data
#this allows for multiple forms of data to be used with varying end dates
n = nrow(y)
#To ensure consistency later in setting Sign Restrictions
#y has variables in the following order
#Unemployment Rate
#Mean Earnings
#Real GDP Per Capita
#Consumer Price Index
#Real Government Consumption Expenditure
#M2 Money Supply
#Change the above as needed to ensure all sign restrictions are on the left


#create VAR model?
#VAR(4) for 1 year of lags

# setup
############################################################

#number of variables
N       = 6
#number of lags
p       = 4
#number of draws
S       = 50000
#not the foggiest
h       = 8


# create Y and X
############################################################  

Y       = ts(y[5:n,])
X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[5:n-i,])
}

t0          = proc.time() # read processor time



# MLE
############################################################
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

#kappa 1 is 1 if non-stationary
#smaller than 1 if stationary
kappa.1     = 1^2
#k2 = 100 is given for Minnesota Prior
kappa.2     = 100
#kappa 3 is 1 just given the fact that it determines A.prior
#which invovles the identity matrix
kappa.3     = 1
A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
#bit in brackets is just 2:N+1
A.prior[2:7,] = kappa.3*diag(N)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1


# normal-inverse Wishard posterior parameters
############################################################

#This is all just given and can be reused infinitely
V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
V.bar       = solve(V.bar.inv)
A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
nu.bar      = nrow(Y) + nu.prior
S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
S.bar.inv   = solve(S.bar)

#Ok so all of that above should get me all the posterior distribution
#Now I need to actually do anything whatsoever with sign restrctions

# Estimating models with sign restrictions: example
# Use Algorithm 2
############################################################
set.seed(123456)
#column by column
sign.restrictions = c(0,1,1,1,1,0)
#this is fine just regardless of how many there are
R1            = diag(sign.restrictions)
#Check how to derive
#I think this is just the posterior
A = A.bar
#check how to derive
#pretty sure this is also just a posterior
Sigma         = S.bar

#Commented out as code contains errors and is non vital
#--------------------------------------------------

#these should be a given I *think*
#error with B0.tilde

#B0.tilde      = t(solve(chol(Sigma)))
#B1.tilde      = B0.tilde%*%A
#IR.0.tilde    = solve(B0.tilde)
#IR.1.tilde    = solve(B0.tilde)%*%B1.tilde%*%solve(B0.tilde)

#This just repeats until I get one where sign restrictions hold
#Assuming the above is all set up properly this shoudl require no more input
#some of this might require a touch of editing
#E,g, X may be based on some given parameters instead of being arbitrary
#sign.restrictions.do.not.hold = TRUE
#i=1
#while (sign.restrictions.do.not.hold){
#  X           = matrix(rnorm(9),3,3)
#  QR          = qr(X, tol = 1e-10)
#  Q           = qr.Q(QR,complete=TRUE)
#  R           = qr.R(QR,complete=TRUE)
#  Q           = t(Q %*% diag(sign(diag(R))))
#  B0          = Q%*%B0.tilde
#  B1          = Q%*%B1.tilde
#  B0.inv      = solve(B0)
#  check       = prod(R1 %*% rbind(B0.inv,B0.inv%*%B1%*%B0.inv) %*% diag(3)[,1] > 0)
#  if (check==1){sign.restrictions.do.not.hold=FALSE}
#  i=i+1
#}
#i
#Q
#B0
#B1
#IR.0        = B0.inv
#IR.1        = B0.inv%*%B1%*%B0.inv
#IR.0
#IR.1
#R1 %*% rbind(IR.0,IR.1) %*% diag(3)[,1]
```

The following is largely taken from Ray's code and provided with consent.

```{r}

#| code-fold: true
#| code-summary: "Show code"

# data = input data should be quarterly
# p = lags
# S = number of posterior draws
# sign restritions = Nx1 diagonal of R matrix
# k1 = kappa1
# k2 = kappa2, higher value, less shrinkage, more weight on prior
# start date = start date of Y matrix

sign.basic <- function(data, p, S,  sign.restrictions,
                           k1=1, k2=100, start_date = c(1979,1), shockvar){
  # Define Y and X matrices
  ############################################################
  # N = no. of variables
  N = ncol(data)
  # p = no. of lags
  K = 1 + p*N
  # forecast horizon
  # h       = 8
  
  Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
  X       = matrix(1,nrow(Y),1)
  # nrow(X)
  for (i in 1:p){
    X     = cbind(X,data[(p+1):nrow(data)-i,])
  }
  
  #t0          = proc.time() # read processor time
  
  # Calculate MLE
  ############################################################
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  # round(A.hat,3)
  # round(Sigma.hat,3)
  # round(cov2cor(Sigma.hat),3)
  
  # Specify prior distribution
  ############################################################
  kappa.1     = k1
  kappa.2     = k2
  kappa.3     = 1
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = kappa.3*diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  # matrix normal-inverse Wishart posterior parameters
  ############################################################
  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
  nu.bar      = nrow(Y) + nu.prior
  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  
  # posterior draws 
  ############################################################
  ## draw from RF posterior
  # draw Sigma from inverse wishart
  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   = apply(Sigma.posterior,3,solve)
  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
  
  # draw A from matrix-variate normal
  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
  
  ## draw from SF posterior
  B0.posterior       = array(NA,c(N,N,S))
  Bplus.posterior       = array(NA,c(N,K,S))
  L                 = t(chol(V.bar))
  for (s in 1:S){
    # Draw B0
    cholSigma.s     = chol(Sigma.posterior[,,s])
    B0.posterior[,,s]= solve(t(cholSigma.s))
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    # Draw Bplus
    Bplus.posterior[,,s] = B0.posterior[,,s]%*%t(A.posterior[,,s])
  }
  # round(apply(A.posterior,1:2,mean),4)
  # round(apply(B.posterior,1:2,mean),4)
  
  # Identification via sign restrictions on theta0
  ############################################################
  # should have N elements
  #sign.restrictions = c()
  
  # generate corresponding R matrix
  R1 = diag(sign.restrictions)
  
  # storage matrices for Q identified estimates
  i.vec <- c()
  Q.iden   = array(NA,c(N,N,S))
  B0.iden = array(NA,c(N,N,S))
  B1.iden = array(NA,c(N,K,S))
  A.iden = array (NA,c(K,N,S))
  
  pb = txtProgressBar(min = 0, max = S, initial = 0) 

  for (s in 1:S){
    
    setTxtProgressBar(pb,s)
    #cat(". iteration: ", s, "\n")
    
    # pick-up a B0 from S
    B0.tilde <- B0.posterior[,,s]
    IR.0.tilde    = solve(B0.tilde)
    B1.tilde      = Bplus.posterior[,,s]
    #IR.1.tilde    = solve(B0.tilde)%*%B1.tilde%*%solve(B0.tilde)

    # Search for appropriate Q 
    sign.restrictions.do.not.hold = TRUE
    i=1
    while (sign.restrictions.do.not.hold){
      X           = matrix(rnorm(N^2),N,N)
      QR          = qr(X, tol = 1e-10)
      Q           = qr.Q(QR,complete=TRUE)
      R           = qr.R(QR,complete=TRUE)
      Q           = t(Q %*% diag(sign(diag(R))))
      B0          = Q%*%B0.tilde
      B1          = Q%*%B1.tilde
      B0.inv      = solve(B0)
      check       = prod(R1 %*% B0.inv %*% diag(N)[,shockvar] >= 0)
      A           = t(solve(B0)%*%B1)

      if (check==1){sign.restrictions.do.not.hold=FALSE}
      i=i+1
    }
    i.vec <- c(i.vec, i)
    Q.iden[,,s] <- Q
    B0.iden[,,s] <- B0
    B0.mean <- apply(B0.iden,1:2,mean)
    B1.iden[,,s] <- B1
    B1.mean <- apply(B1.iden,1:2,mean)
    A.iden[,,s] <- A
    A.mean <- apply(A.iden,1:2,mean)
    

  }
  re <- list("i" = i.vec, "Q" = Q.iden, "B0"= B0.iden, "B0.mean" = B0.mean,
             "Bplus"= B1.iden, "Bplus.mean" = B1.mean, "A" = A.iden, "A.mean" = A,
             "A.posterior"=A.posterior, "Sigma.posterior"=Sigma.posterior)
  return(re)
}

set.seed(69420)
RW_res <-  sign.basic(y, p, S=1000, R1, k1 = 1, k2 = 100, shockvar = 1)
round(RW_res$A.mean,1)

basic.model = RW_res

N <- dim(basic.model$A.posterior)[2]

S <- dim(basic.model$A.posterior)[3]
p <- 4
h <- 12



A.posterior.basic <- basic.model$A.posterior

B0.posterior.basic <- basic.model$B0

B.posterior.basic <- array(NA, c(N,N,S))

for (s in 1:S){
  B.posterior.basic[,,s] <- solve(B0.posterior.basic[,,s])
}


A.posterior <- A.posterior.basic
B.posterior <- B.posterior.basic

IRF.posterior     = array(NA,c(N,N,h+1,S))
IRF.inf.posterior = array(NA,c(N,N,S))
FEVD.posterior    = array(NA,c(N,N,h+1,S))
J                 = cbind(diag(N),matrix(0,N,N*(p-1)))

pb = txtProgressBar(min = 0, max = S, initial = 0) 

for (s in 1:S){
  setTxtProgressBar(pb,s)
  A.bold          = rbind(t(A.posterior[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
  IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]
  A.bold.power    = A.bold
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior[,,i,s]        = B.posterior[,,s]
    } else {
      IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% B.posterior[,,s]
      A.bold.power                = A.bold.power %*% A.bold
    }
    for (n in 1:N){
      for (nn in 1:N){
        FEVD.posterior[n,nn,i,s]  = sum(IRF.posterior[n,nn,1:i,s]^2)
      }
    }
    FEVD.posterior[,,i,s]         = diag(1/apply(FEVD.posterior[,,i,s],1,sum))%*%FEVD.posterior[,,i,s]
  }
}
FEVD.posterior    = 100*FEVD.posterior

varname_vec <- c("Weekly Earnings", "Unemployment Rate", "Real GDP Per Capita", "CPI", "Government Spending", "M2 Money Supply")

shock.var <- 1
IRFs.k1           = apply(IRF.posterior[1:6,shock.var,,],1:2,mean)
IRFs.inf.k1       = apply(IRF.inf.posterior[1:6,shock.var,],1,mean)
rownames(IRFs.k1) = varname_vec

IRFs.k1.hdi    = apply(IRF.posterior[1:6,shock.var,,],1:2,hdi, credMass=0.68)
hh          = 1:(h+1)

#This needs to look prettier
#hmm. My graphs are not rendering correctly

#pdf(file="irf-GFCF.pdf", height=9, width=12)
par(mfrow=c(3,3), mar=c(3,3,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="3 years",
       main=rownames(IRFs.k1)[n])
  abline(h = 0, col = "firebrick")
  if (n==N-1 | n==N){
   # axis(1,c(1,2,5,9),c("","1 quarter","1 year","2 years"))
  } else {
    #axis(1,c(1,2,5,9),c("","","",""))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col="#ccffff",border="salmon")
  lines(hh, IRFs.k1[n,hh],lwd=2,col="red")
}
```

##Analysis

As can be seen, when weekly earnings is shocked, it tends to cause a shock in the opposite direction in the next period, followed by a small increase, and then stabilizing such that by the end of the second year an income shock creates no further changes in the economy. Curiously, unemployment rate also falls and RGDP experiences a spike before returning to no continuing effect. M2 Money Supply is decreased and suffers from a decreased future growth as well.

CPI and Government Expenditure show no significant effects in this model.

One possible explanation for these changes seeming contradictory to initial assumptions is the holiday worker rush and holiday bonuses interacting with the data. This seasonality should be controlled for by the lag order but I can think of no other viable explanation. In the United States, there is a significant upsurge in employment during the time right around Christmas in December, both in the lead up and during Christmas. During this time is also when many bonuses are paid to workers. And an upsurge in employment is consistent with a spike in RGDP per capita.

Another possible reason is the volatility spike from COVID completely subsuming all other effects so that the real effects cannot be observed. 

Further analysis is needed as I am unclear why these results are occurring in what should be the absence of seasonality and sign restrictions. To test this I will create a second set of Impulse Response Functions including only pre-COVID effects. This will also help me tell what form I expect my IRFs to take after adjusting for COVID

## References {.unnumbered}
